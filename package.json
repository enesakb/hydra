// ============================================================================
// HYDRA — AI Model Router
// Intelligently selects model based on survival tier, cost, and task complexity
// ============================================================================

import { Logger } from '../state/logger.js';
import { SurvivalMonitor } from '../survival/monitor.js';
import { SurvivalTier, type ModelConfig, type ModelResponse, type Message } from '../core/types.js';

// Available models ordered by capability (best → cheapest)
const MODEL_REGISTRY: ModelConfig[] = [
  {
    provider: 'anthropic',
    model: 'claude-opus-4-20250514',
    maxTokens: 8192,
    costPer1kInput: 0.015,
    costPer1kOutput: 0.075,
    tier: [SurvivalTier.APEX],
  },
  {
    provider: 'anthropic',
    model: 'claude-sonnet-4-20250514',
    maxTokens: 8192,
    costPer1kInput: 0.003,
    costPer1kOutput: 0.015,
    tier: [SurvivalTier.APEX, SurvivalTier.THRIVING, SurvivalTier.STABLE],
  },
  {
    provider: 'openai',
    model: 'gpt-4o',
    maxTokens: 4096,
    costPer1kInput: 0.005,
    costPer1kOutput: 0.015,
    tier: [SurvivalTier.APEX, SurvivalTier.THRIVING, SurvivalTier.STABLE],
  },
  {
    provider: 'anthropic',
    model: 'claude-haiku-4-20250414',
    maxTokens: 4096,
    costPer1kInput: 0.0008,
    costPer1kOutput: 0.004,
    tier: [SurvivalTier.APEX, SurvivalTier.THRIVING, SurvivalTier.STABLE, SurvivalTier.CONSERVING],
  },
  {
    provider: 'openai',
    model: 'gpt-4o-mini',
    maxTokens: 4096,
    costPer1kInput: 0.00015,
    costPer1kOutput: 0.0006,
    tier: [SurvivalTier.APEX, SurvivalTier.THRIVING, SurvivalTier.STABLE, SurvivalTier.CONSERVING, SurvivalTier.CRITICAL],
  },
  {
    provider: 'local',
    model: 'llama-3.1-8b',
    maxTokens: 2048,
    costPer1kInput: 0,
    costPer1kOutput: 0,
    tier: [SurvivalTier.APEX, SurvivalTier.THRIVING, SurvivalTier.STABLE, SurvivalTier.CONSERVING, SurvivalTier.CRITICAL],
  },
];

interface CompletionRequest {
  system: string;
  messages: Message[];
  maxTokens?: number;
  preferredModel?: string;
}

export class ModelRouter {
  private survival: SurvivalMonitor;
  private logger: Logger;
  private totalSpent: number = 0;

  constructor(survival: SurvivalMonitor, logger: Logger) {
    this.survival = survival;
    this.logger = logger;
  }

  /**
   * Route completion to the best available model for current survival tier.
   */
  async complete(request: CompletionRequest): Promise<ModelResponse> {
    const tier = (await this.survival.evaluate()).tier;
    const model = this.selectModel(tier, request.preferredModel);

    if (!model) {
      throw new Error(`No model available for tier: ${tier}`);
    }

    this.logger.debug(`Routing to ${model.provider}/${model.model} (tier: ${tier})`);

    const startTime = Date.now();

    try {
      const response = await this.callProvider(model, request);
      const latencyMs = Date.now() - startTime;

      const costUSD =
        (response.inputTokens / 1000) * model.costPer1kInput +
        (response.outputTokens / 1000) * model.costPer1kOutput;

      this.totalSpent += costUSD;

      return {
        content: response.content,
        inputTokens: response.inputTokens,
        outputTokens: response.outputTokens,
        costUSD,
        latencyMs,
        model: model.model,
      };
    } catch (error) {
      this.logger.error(`Model ${model.model} failed, trying fallback`, { error });

      // Try fallback model
      const fallback = this.selectFallbackModel(tier, model.model);
      if (fallback) {
        return this.callProviderWithMetrics(fallback, request);
      }

      throw error;
    }
  }

  /**
   * Select the best model for the current tier.
   */
  private selectModel(tier: SurvivalTier, preferred?: string): ModelConfig | null {
    // Try preferred model first
    if (preferred) {
      const pref = MODEL_REGISTRY.find(
        m => m.model === preferred && m.tier.includes(tier)
      );
      if (pref) return pref;
    }

    // Get all models available for this tier, sorted by capability (first = best)
    const available = MODEL_REGISTRY.filter(m => m.tier.includes(tier));

    if (available.length === 0) return null;

    // In critical tier, always pick cheapest
    if (tier === SurvivalTier.CRITICAL) {
      return available[available.length - 1];
    }

    // In conserving tier, pick second cheapest
    if (tier === SurvivalTier.CONSERVING) {
      return available[Math.max(0, available.length - 2)];
    }

    // Otherwise, pick the best available
    return available[0];
  }

  private selectFallbackModel(tier: SurvivalTier, excludeModel: string): ModelConfig | null {
    const available = MODEL_REGISTRY.filter(
      m => m.tier.includes(tier) && m.model !== excludeModel
    );
    return available.length > 0 ? available[available.length - 1] : null;
  }

  /**
   * Call a specific AI provider.
   */
  private async callProvider(
    model: ModelConfig,
    request: CompletionRequest
  ): Promise<{ content: string; inputTokens: number; outputTokens: number }> {
    switch (model.provider) {
      case 'anthropic':
        return this.callAnthropic(model, request);
      case 'openai':
        return this.callOpenAI(model, request);
      case 'google':
        return this.callGoogle(model, request);
      case 'local':
        return this.callLocal(model, request);
      default:
        throw new Error(`Unknown provider: ${model.provider}`);
    }
  }

  private async callProviderWithMetrics(
    model: ModelConfig,
    request: CompletionRequest
  ): Promise<ModelResponse> {
    const startTime = Date.now();
    const response = await this.callProvider(model, request);
    const latencyMs = Date.now() - startTime;
    const costUSD =
      (response.inputTokens / 1000) * model.costPer1kInput +
      (response.outputTokens / 1000) * model.costPer1kOutput;
    this.totalSpent += costUSD;

    return {
      ...response,
      costUSD,
      latencyMs,
      model: model.model,
    };
  }

  // ─── Provider Implementations ────────────────────────────────────────────

  private async callAnthropic(
    model: ModelConfig,
    request: CompletionRequest
  ): Promise<{ content: string; inputTokens: number; outputTokens: number }> {
    const apiKey = process.env.ANTHROPIC_API_KEY;
    if (!apiKey) throw new Error('ANTHROPIC_API_KEY not set');

    const { default: Anthropic } = await import('@anthropic-ai/sdk');
    const client = new Anthropic({ apiKey });

    const messages = request.messages.map(m => ({
      role: m.role === 'system' ? 'user' as const : m.role as 'user' | 'assistant',
      content: m.content,
    }));

    const response = await client.messages.create({
      model: model.model,
      max_tokens: request.maxTokens || model.maxTokens,
      system: request.system,
      messages,
    });

    const content = response.content
      .filter(block => block.type === 'text')
      .map(block => (block as any).text)
      .join('');

    return {
      content,
      inputTokens: response.usage.input_tokens,
      outputTokens: response.usage.output_tokens,
    };
  }

  private async callOpenAI(
    model: ModelConfig,
    request: CompletionRequest
  ): Promise<{ content: string; inputTokens: number; outputTokens: number }> {
    const apiKey = process.env.OPENAI_API_KEY;
    if (!apiKey) throw new Error('OPENAI_API_KEY not set');

    const { default: OpenAI } = await import('openai');
    const client = new OpenAI({ apiKey });

    const messages = [
      { role: 'system' as const, content: request.system },
      ...request.messages.map(m => ({
        role: m.role as 'user' | 'assistant' | 'system',
        content: m.content,
      })),
    ];

    const response = await client.chat.completions.create({
      model: model.model,
      max_tokens: request.maxTokens || model.maxTokens,
      messages,
    });

    return {
      content: response.choices[0]?.message?.content || '',
      inputTokens: response.usage?.prompt_tokens || 0,
      outputTokens: response.usage?.completion_tokens || 0,
    };
  }

  private async callGoogle(
    model: ModelConfig,
    request: CompletionRequest
  ): Promise<{ content: string; inputTokens: number; outputTokens: number }> {
    const apiKey = process.env.GOOGLE_API_KEY;
    if (!apiKey) throw new Error('GOOGLE_API_KEY not set');

    const { GoogleGenerativeAI } = await import('@google/generative-ai');
    const genAI = new GoogleGenerativeAI(apiKey);
    const genModel = genAI.getGenerativeModel({ model: model.model });

    const prompt = `${request.system}\n\n${request.messages.map(m => `${m.role}: ${m.content}`).join('\n')}`;
    const result = await genModel.generateContent(prompt);

    return {
      content: result.response.text(),
      inputTokens: 0, // Google doesn't always return token counts
      outputTokens: 0,
    };
  }

  private async callLocal(
    _model: ModelConfig,
    request: CompletionRequest
  ): Promise<{ content: string; inputTokens: number; outputTokens: number }> {
    // Ollama API
    const baseUrl = process.env.OLLAMA_URL || 'http://localhost:11434';

    try {
      const response = await fetch(`${baseUrl}/api/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          model: 'llama3.1:8b',
          messages: [
            { role: 'system', content: request.system },
            ...request.messages.map(m => ({ role: m.role, content: m.content })),
          ],
          stream: false,
        }),
      });

      const data = await response.json() as any;
      return {
        content: data.message?.content || '',
        inputTokens: data.prompt_eval_count || 0,
        outputTokens: data.eval_count || 0,
      };
    } catch {
      this.logger.warn('Local model (Ollama) unavailable');
      return { content: 'Local model unavailable. Minimal operation mode.', inputTokens: 0, outputTokens: 0 };
    }
  }

  // ─── Analytics ───────────────────────────────────────────────────────────

  getTotalSpent(): number {
    return this.totalSpent;
  }

  getAvailableModels(tier: SurvivalTier): ModelConfig[] {
    return MODEL_REGISTRY.filter(m => m.tier.includes(tier));
  }
}
